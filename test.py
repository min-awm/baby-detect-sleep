import cv2
import torch
import time
import numpy as np
import mediapipe as mp
from datetime import datetime
import os
import shutil
import warnings
from collections import deque
import math

from scipy.spatial.distance import cosine
import threading
import queue
import json

# Import c√°c module t·ª± t·∫°o
from ArcFace import ArcFace
from stream import connect_to_stream
from drive.google_drive import upload_to_drive
from config import ESP32_STREAM_URL, GOOGLE_DRIVE_FOLDER_ID

warnings.filterwarnings("ignore", category=FutureWarning)

# Th·ªùi gian b·∫Øt ƒë·∫ßu h·ªá th·ªëng
start_time = time.time()


# ============ SYSTEM CONFIGURATION ============
class Config:
    # C·∫•u h√¨nh x·ª≠ l√Ω frame
    FRAME_SKIP = 2
    POSE_CONFIDENCE_THRESHOLD = 0.7
    BABY_SIZE_THRESHOLD = 0.15
    POSE_HISTORY_SIZE = 7
    FACE_PROCESS_INTERVAL = 10  # X·ª≠ l√Ω khu√¥n m·∫∑t m·ªói X frame

    # Ng∆∞·ª°ng ph√°t hi·ªán ng∆∞·ªùi l·∫° (cosine similarity)
    # Gi√° tr·ªã cao h∆°n = y√™u c·∫ßu khu√¥n m·∫∑t ph·∫£i gi·ªëng h∆°n ƒë·ªÉ kh√¥ng b·ªã coi l√† ng∆∞·ªùi l·∫°
    STRANGER_SIMILARITY_THRESHOLD = 0.6

    # Ng∆∞·ª°ng g√≥c t∆∞ th·∫ø
    TUMMY_TIME_ANGLE_THRESHOLD = 15
    INVERTED_THRESHOLD = 160  # Cho t∆∞ th·∫ø √∫p m·∫∑t nguy hi·ªÉm

    # Kho·∫£ng th·ªùi gian c·∫£nh b√°o (gi√¢y)
    ALERT_INTERVAL = 30

    # Th∆∞ m·ª•c l∆∞u tr·ªØ
    SNAPSHOTS_DIR = "snapshots"
    KNOWN_FACES_DIR = "known_faces"
    LOGS_DIR = "logs"


config = Config()


# ============ KNOWN FACES LOADER ============
def load_known_faces():
    """T·∫£i c√°c khu√¥n m·∫∑t ƒë√£ bi·∫øt s·ª≠ d·ª•ng ArcFace embeddings."""
    known_encodings = []
    known_names = []

    if not os.path.exists(config.KNOWN_FACES_DIR):
        print(f"‚ö†Ô∏è Th∆∞ m·ª•c {config.KNOWN_FACES_DIR} kh√¥ng t·ªìn t·∫°i")
        return known_encodings, known_names

    # Kh·ªüi t·∫°o ArcFace model ƒë·ªÉ extract embeddings
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    arcface_temp = ArcFace(device=device)

    if arcface_temp.app is None:
        print("‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o ArcFace ƒë·ªÉ load faces")
        return known_encodings, known_names

    for filename in os.listdir(config.KNOWN_FACES_DIR):
        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            name = os.path.splitext(filename)[0]
            filepath = os.path.join(config.KNOWN_FACES_DIR, filename)

            try:
                # ƒê·ªçc ·∫£nh
                image = cv2.imread(filepath)
                if image is None:
                    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {filepath}")
                    continue

                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

                # Extract embedding s·ª≠ d·ª•ng ArcFace
                embedding = arcface_temp.get_face_embedding(image_rgb)

                if embedding is not None:
                    known_encodings.append(embedding)
                    known_names.append(name)
                    print(f"‚úÖ ƒê√£ t·∫£i khu√¥n m·∫∑t: {name}")
                else:
                    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ tr√≠ch xu·∫•t embedding cho: {name}")

            except Exception as e:
                print(f"‚ùå L·ªói x·ª≠ l√Ω ·∫£nh {filename}: {e}")

    print(f"‚úÖ ƒê√£ t·∫£i {len(known_encodings)} khu√¥n m·∫∑t ƒë√£ bi·∫øt")
    return known_encodings, known_names


def recognize_face_arcface(face_image, known_encodings, known_names, arcface_model, threshold=0.6):
    """
    Nh·∫≠n di·ªán khu√¥n m·∫∑t s·ª≠ d·ª•ng ArcFace embeddings v√† cosine similarity.

    Args:
        face_image: ·∫¢nh khu√¥n m·∫∑t (RGB format)
        known_encodings: List c√°c embeddings ƒë√£ bi·∫øt
        known_names: List t√™n t∆∞∆°ng ·ª©ng
        arcface_model: ArcFace model instance
        threshold: Ng∆∞·ª°ng cosine similarity

    Returns:
        (name, confidence) ho·∫∑c (None, 0) n·∫øu kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c
    """
    if not known_encodings or arcface_model is None or arcface_model.app is None:
        return None, 0

    try:
        # Extract embedding t·ª´ khu√¥n m·∫∑t input
        current_embedding = arcface_model.get_face_embedding(face_image)

        if current_embedding is None:
            return None, 0

        best_match_idx = None
        best_similarity = 0

        # So s√°nh v·ªõi t·∫•t c·∫£ embeddings ƒë√£ bi·∫øt
        for i, known_embedding in enumerate(known_encodings):
            # T√≠nh cosine similarity (1 - cosine distance)
            similarity = 1 - cosine(current_embedding, known_embedding)

            if similarity > best_similarity and similarity > threshold:
                best_similarity = similarity
                best_match_idx = i

        if best_match_idx is not None:
            return known_names[best_match_idx], best_similarity
        else:
            return None, 0

    except Exception as e:
        print(f"‚ùå L·ªói nh·∫≠n di·ªán khu√¥n m·∫∑t: {e}")
        return None, 0


# ============ FACE DETECTION CLASS ============
class FaceDetector:
    """Ph√°t hi·ªán khu√¥n m·∫∑t s·ª≠ d·ª•ng OpenCV DNN ho·∫∑c Haar Cascade."""

    def __init__(self, confidence_threshold=0.5):
        self.confidence_threshold = confidence_threshold
        self.use_dnn = False

        # Th·ª≠ s·ª≠ d·ª•ng DNN detector tr∆∞·ªõc
        try:
            # ƒê∆∞·ªùng d·∫´n ƒë·∫øn DNN models - ƒë·∫£m b·∫£o c√°c file n√†y c√≥ m·∫∑t
            dnn_model = 'opencv_face_detector_uint8.pb'
            dnn_config = 'opencv_face_detector.pbtxt'

            if not os.path.exists(dnn_model) or not os.path.exists(dnn_config):
                print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file DNN face detector: {dnn_model}, {dnn_config}")
                raise FileNotFoundError

            self.net = cv2.dnn.readNetFromTensorflow(dnn_model, dnn_config)
            self.use_dnn = True
            print("‚úÖ S·ª≠ d·ª•ng DNN face detector")
        except Exception as e:
            print(f"‚ö†Ô∏è DNN face detector th·∫•t b·∫°i: {e}")
            # Fallback v·ªÅ Haar cascade
            haar_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            if not os.path.exists(haar_cascade_path):
                print(f"‚ùå Kh√¥ng t√¨m th·∫•y file Haar cascade: {haar_cascade_path}")
                print("Vui l√≤ng ƒë·∫£m b·∫£o OpenCV ƒë∆∞·ª£c c√†i ƒë·∫∑t ƒë·∫ßy ƒë·ªß.")
                self.face_cascade = None
            else:
                self.face_cascade = cv2.CascadeClassifier(haar_cascade_path)
            self.use_dnn = False
            print("‚úÖ S·ª≠ d·ª•ng Haar cascade face detector")

    def detect_faces(self, image):
        """Ph√°t hi·ªán khu√¥n m·∫∑t trong ·∫£nh."""
        if self.use_dnn and hasattr(self, 'net'):
            return self._detect_faces_dnn(image)
        elif not self.use_dnn and hasattr(self, 'face_cascade') and self.face_cascade is not None:
            return self._detect_faces_haar(image)
        else:
            return []  # Kh√¥ng c√≥ detector ƒë∆∞·ª£c kh·ªüi t·∫°o

    def _detect_faces_dnn(self, image):
        """Ph√°t hi·ªán khu√¥n m·∫∑t s·ª≠ d·ª•ng DNN."""
        h, w = image.shape[:2]
        blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123], False, False)
        self.net.setInput(blob)
        detections = self.net.forward()

        faces = []
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]
            if confidence > self.confidence_threshold:
                x1 = int(detections[0, 0, i, 3] * w)
                y1 = int(detections[0, 0, i, 4] * h)
                x2 = int(detections[0, 0, i, 5] * w)
                y2 = int(detections[0, 0, i, 6] * h)
                faces.append((x1, y1, x2 - x1, y2 - y1))
        return faces

    def _detect_faces_haar(self, image):
        """Ph√°t hi·ªán khu√¥n m·∫∑t s·ª≠ d·ª•ng Haar cascade."""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(
            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)
        )
        return faces.tolist() if len(faces) > 0 else []


# ============ YOLO MODEL LOADER ============
class YOLODetector:
    """Class ph√°t hi·ªán ng∆∞·ªùi s·ª≠ d·ª•ng YOLO."""

    def __init__(self):
        self.model = None
        self.load_model()

    def clear_torch_cache(self):
        """X√≥a torch cache ƒë·ªÉ tr√°nh xung ƒë·ªôt."""
        try:
            cache_dir = torch.hub.get_dir()
            if os.path.exists(cache_dir):
                for item in os.listdir(cache_dir):
                    item_path = os.path.join(cache_dir, item)
                    if 'yolov5' in item.lower():
                        if os.path.isdir(item_path):
                            shutil.rmtree(item_path)
                        elif os.path.isfile(item_path):
                            os.remove(item_path)
            print("‚úÖ ƒê√£ x√≥a torch hub cache")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a cache: {e}")

    def load_model(self):
        """T·∫£i YOLO model."""
        print("üîÑ ƒêang t·∫£i YOLO model t·ªëi ∆∞u...")
        try:
            from ultralytics import YOLO
            # ƒê·∫£m b·∫£o yolov5s.pt ƒë∆∞·ª£c t·∫£i v·ªÅ ho·∫∑c c√≥ s·∫µn locally
            model = YOLO('yolov5s.pt')
            model.classes = [0]  # Ch·ªâ class 'person'

            class OptimizedYOLOWrapper:
                def __init__(self, model_instance):
                    self.model = model_instance
                    self.classes = [0]

                def __call__(self, frame):
                    # ƒê·∫∑t verbose=False ƒë·ªÉ t·∫Øt output trong qu√° tr√¨nh inference
                    results = self.model(
                        frame,
                        classes=self.classes,
                        verbose=False,
                        imgsz=416,  # Ho·∫∑c 640, ƒëi·ªÅu ch·ªânh ƒë·ªÉ c√¢n b·∫±ng hi·ªáu su·∫•t/ƒë·ªô ch√≠nh x√°c
                        conf=0.4,
                        iou=0.5
                    )

                    detections = []
                    # ultralytics YOLO tr·∫£ v·ªÅ danh s√°ch Results objects
                    for r in results:
                        boxes = r.boxes  # Boxes object ch·ª©a d·ªØ li·ªáu bounding box
                        if boxes is not None:
                            # xyxy cho (x1, y1, x2, y2)
                            # conf cho confidence score
                            # cls cho class id
                            for i in range(len(boxes)):
                                x1, y1, x2, y2 = boxes.xyxy[i].cpu().numpy().astype(int)
                                conf = boxes.conf[i].cpu().numpy()
                                cls = boxes.cls[i].cpu().numpy()
                                detections.append([x1, y1, x2, y2, conf, cls])

                    # M√¥ ph·ªèng format results c≈© c·ªßa YOLOv5 ƒë·ªÉ t∆∞∆°ng th√≠ch
                    class MockResults:
                        def __init__(self, detections_list):
                            # Chuy·ªÉn ƒë·ªïi th√†nh numpy array n·∫øu detections_list kh√¥ng r·ªóng
                            self.xyxy = [np.array(detections_list) if detections_list else np.empty((0, 6))]

                    return MockResults(detections)

            self.model = OptimizedYOLOWrapper(model)
            print("‚úÖ YOLO model ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")

        except Exception as e:
            print(f"‚ùå Kh√¥ng th·ªÉ t·∫£i YOLO model: {e}")
            self.model = None


# ============ BABY POSE CLASSIFIER CLASS ============
class EnhancedBabyPoseClassifier:
    """Ph√¢n lo·∫°i t∆∞ th·∫ø em b√© n√¢ng cao."""

    def __init__(self):
        self.pose_history = deque(maxlen=config.POSE_HISTORY_SIZE)
        self.stable_pose = "khong_xac_dinh"
        self.confidence_threshold = 0.6
        self.angle_history = deque(maxlen=5)

    def calculate_angle(self, p1, p2, p3):
        """T√≠nh g√≥c gi·ªØa 3 ƒëi·ªÉm."""
        try:
            v1 = np.array([p1.x - p2.x, p1.y - p2.y])
            v2 = np.array([p3.x - p2.x, p3.y - p2.y])

            cosine_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
            cosine_angle = np.clip(cosine_angle, -1.0, 1.0)
            angle = np.arccos(cosine_angle)

            return np.degrees(angle)
        except:
            return 0

    def calculate_body_orientation(self, landmarks):
        """T√≠nh h∆∞·ªõng c∆° th·ªÉ."""
        try:
            # L·∫•y c√°c ƒëi·ªÉm landmark quan tr·ªçng
            nose = landmarks[mp.solutions.pose.PoseLandmark.NOSE.value]
            left_shoulder = landmarks[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER.value]
            right_shoulder = landmarks[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER.value]
            left_hip = landmarks[mp.solutions.pose.PoseLandmark.LEFT_HIP.value]
            right_hip = landmarks[mp.solutions.pose.PoseLandmark.RIGHT_HIP.value]
            left_ear = landmarks[mp.solutions.pose.PoseLandmark.LEFT_EAR.value]
            right_ear = landmarks[mp.solutions.pose.PoseLandmark.RIGHT_EAR.value]
            left_eye = landmarks[mp.solutions.pose.PoseLandmark.LEFT_EYE.value]
            right_eye = landmarks[mp.solutions.pose.PoseLandmark.RIGHT_EYE.value]

            # T√≠nh c√°c ƒëi·ªÉm trung t√¢m
            shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
            shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
            hip_center_x = (left_hip.x + right_hip.x) / 2
            hip_center_y = (left_hip.y + right_hip.y) / 2
            ear_center_y = (left_ear.y + right_ear.y) / 2
            eye_center_y = (left_eye.y + right_eye.y) / 2

            # T√≠nh vector c∆° th·ªÉ (t·ª´ vai ƒë·∫øn h√¥ng)
            body_vector_x = hip_center_x - shoulder_center_x
            body_vector_y = hip_center_y - shoulder_center_y
            body_angle = math.atan2(body_vector_y, body_vector_x)
            body_angle_degrees = abs(math.degrees(body_angle))

            # C√°c tham s·ªë ph√¢n t√≠ch kh√°c
            nose_to_ear_y = nose.y - ear_center_y
            nose_to_eye_y = nose.y - eye_center_y
            shoulder_diff = abs(left_shoulder.y - right_shoulder.y)  # Ch√™nh l·ªách d·ªçc gi·ªØa c√°c vai

            return {
                'body_angle_degrees': body_angle_degrees,
                'nose_to_ear_y': nose_to_ear_y,
                'nose_to_eye_y': nose_to_eye_y,
                'shoulder_diff': shoulder_diff,
                'shoulder_center_y': shoulder_center_y,
                'hip_center_y': hip_center_y,
                'nose_y': nose.y,
                'body_vertical': abs(body_angle_degrees - 90) < 30,  # G√≥c g·∫ßn 90 ƒë·ªô (d·ªçc)
                'body_horizontal': body_angle_degrees < 30 or body_angle_degrees > 150  # G√≥c g·∫ßn 0 ho·∫∑c 180 (ngang)
            }
        except Exception as e:
            return None

    def classify_pose_enhanced(self, landmarks, image_width, image_height):
        """Ph√¢n lo·∫°i t∆∞ th·∫ø n√¢ng cao."""
        orientation = self.calculate_body_orientation(landmarks)
        if not orientation:
            return "khong_xac_dinh", 0.0

        confidence = 0.0
        pose_label = "khong_xac_dinh"

        # L∆∞u tr·ªØ l·ªãch s·ª≠ g√≥c ƒë·ªÉ ·ªïn ƒë·ªãnh
        self.angle_history.append(orientation['body_angle_degrees'])
        avg_angle = sum(self.angle_history) / len(self.angle_history) if self.angle_history else 0

        # Ph√¢n lo·∫°i t∆∞ th·∫ø
        # N·∫±m ng·ª≠a (On back)
        if (orientation['nose_to_ear_y'] > 0.07 and  # M≈©i cao h∆°n ƒë∆∞·ªùng tai ƒë√°ng k·ªÉ
                orientation['shoulder_diff'] > 0.03 and  # Vai t∆∞∆°ng ƒë·ªëi ngang
                orientation['body_horizontal'] and  # C∆° th·ªÉ n·∫±m ngang
                abs(avg_angle) < 10):  # G√≥c c∆° th·ªÉ g·∫ßn ngang
            pose_label = "nam_ngua"
            confidence = min(0.95, 0.75 + abs(orientation['nose_to_ear_y']) * 5)

        # Tummy Time (N·∫±m s·∫•p, ƒë·∫ßu n√¢ng l√™n)
        elif (orientation['nose_to_ear_y'] < 0.05 and orientation[
            'nose_to_ear_y'] > -0.05 and  # M≈©i xung quanh ƒë∆∞·ªùng tai
              orientation['nose_to_eye_y'] > 0.03 and  # M≈©i d∆∞·ªõi ƒë∆∞·ªùng m·∫Øt (ƒë·∫ßu ng·∫©ng l√™n)
              orientation['body_horizontal'] and  # C∆° th·ªÉ n·∫±m ngang
              orientation['shoulder_center_y'] < orientation['hip_center_y'] and  # Vai cao h∆°n h√¥ng
              abs(avg_angle) < 15):  # G√≥c c∆° th·ªÉ g·∫ßn ngang
            pose_label = "tummy_time"
            confidence = min(0.9, 0.7 + abs(orientation['nose_to_ear_y'] - 0.005) * 10)

        # N·∫±m s·∫•p (On stomach, √∫p m·∫∑t)
        elif (orientation['nose_to_ear_y'] < -0.05 and  # M≈©i th·∫•p h∆°n ƒë∆∞·ªùng tai ƒë√°ng k·ªÉ
              orientation['body_horizontal'] and  # C∆° th·ªÉ n·∫±m ngang
              orientation['shoulder_diff'] < 0.1 and  # Vai t∆∞∆°ng ƒë·ªëi n·∫±m ngang
              abs(avg_angle) < 10):  # G√≥c c∆° th·ªÉ g·∫ßn ngang
            pose_label = "nam_sap"
            confidence = min(0.95, 0.65 + abs(orientation['nose_to_ear_y']) * 8)

        # N·∫±m √∫p 180 (Inverted/Nguy hi·ªÉm - ƒë·∫ßu ho√†n to√†n h∆∞·ªõng xu·ªëng)
        elif (orientation['body_horizontal'] and  # C∆° th·ªÉ n·∫±m ngang
              orientation['hip_center_y'] < orientation['shoulder_center_y'] and  # H√¥ng cao h∆°n vai (√∫p ng∆∞·ª£c)
              abs(avg_angle) > 170):  # G√≥c c∆° th·ªÉ g·∫ßn 180 ƒë·ªô
            pose_label = "nam_up_180"  # nguy hi·ªÉm
            confidence = min(0.95, 0.7 + abs(orientation['nose_to_ear_y']) * 8)

        return pose_label, confidence

    def update_pose_history(self, pose_label, confidence):
        """C·∫≠p nh·∫≠t l·ªãch s·ª≠ t∆∞ th·∫ø."""
        if confidence > self.confidence_threshold:
            self.pose_history.append((pose_label, confidence, time.time()))

        # X√≥a d·ªØ li·ªáu c≈©
        current_time = time.time()
        self.pose_history = deque(
            [(pose, conf, t) for pose, conf, t in self.pose_history if current_time - t < 3.0],  # Gi·ªØ 3 gi√¢y g·∫ßn nh·∫•t
            maxlen=config.POSE_HISTORY_SIZE
        )

        # X√°c ƒë·ªãnh t∆∞ th·∫ø ·ªïn ƒë·ªãnh
        if len(self.pose_history) >= 3:  # C·∫ßn √≠t nh·∫•t 3 entry g·∫ßn ƒë√¢y ƒë·ªÉ ·ªïn ƒë·ªãnh
            recent_poses = [(p[0], p[1]) for p in list(self.pose_history)[-3:]]
            pose_counts = {}

            for pose, conf in recent_poses:
                if pose not in pose_counts:
                    pose_counts[pose] = []
                pose_counts[pose].append(conf)

            best_pose = None
            best_score = 0

            for pose, confidences in pose_counts.items():
                avg_conf = sum(confidences) / len(confidences)
                # Score t√≠nh c·∫£ confidence trung b√¨nh v√† t·∫ßn su·∫•t
                score = avg_conf * (len(confidences) / len(recent_poses))

                if score > best_score:
                    best_score = score
                    best_pose = pose

            if best_pose and best_score > 0.4:  # Score h·ª£p l√Ω ƒë·ªÉ coi l√† ·ªïn ƒë·ªãnh
                self.stable_pose = best_pose
                return self.stable_pose, best_score

        return self.stable_pose, 0.5  # Tr·∫£ v·ªÅ stable hi·ªán t·∫°i ho·∫∑c default n·∫øu ch∆∞a ƒë·ªß l·ªãch s·ª≠


# ============ UTILITY FUNCTIONS ============
def is_baby_size(box, frame_width, frame_height):
    """Ki·ªÉm tra bounding box c√≥ trong kho·∫£ng k√≠ch th∆∞·ªõc em b√© kh√¥ng."""
    x1, y1, x2, y2 = box
    box_width = x2 - x1
    box_height = y2 - y1
    box_area = box_width * box_height
    frame_area = frame_width * frame_height
    relative_size = box_area / frame_area
    # ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng n√†y d·ª±a tr√™n k√≠ch th∆∞·ªõc em b√© th√¥ng th∆∞·ªùng so v·ªõi g√≥c nh√¨n camera
    return 0.01 <= relative_size <= 0.25  # Gi·∫£m k√≠ch th∆∞·ªõc t·ªëi ƒëa ƒë·ªÉ ph√π h·ª£p v·ªõi em b√©


def is_child_face(face_location, baby_boxes, margin=20):
    """Ki·ªÉm tra khu√¥n m·∫∑t c√≥ ph·∫£i c·ªßa em b√© kh√¥ng."""
    x, y, w, h = face_location
    face_center_x = x + w // 2
    face_center_y = y + h // 2

    for bx1, by1, bx2, by2 in baby_boxes:
        # Ki·ªÉm tra t√¢m khu√¥n m·∫∑t c√≥ n·∫±m trong bounding box em b√© v·ªõi margin kh√¥ng
        if (bx1 - margin <= face_center_x <= bx2 + margin and
                by1 - margin <= face_center_y <= by2 + margin):
            return True
    return False


def save_alert_snapshot(frame, alert_type, timestamp):
    """L∆∞u snapshot c·∫£nh b√°o."""
    try:
        os.makedirs(config.SNAPSHOTS_DIR, exist_ok=True)
        filename = f"{alert_type}_{int(timestamp)}.jpg"
        filepath = os.path.join(config.SNAPSHOTS_DIR, filename)
        cv2.imwrite(filepath, frame)
        print(f"üì∏ ƒê√£ l∆∞u snapshot: {filepath}")
        return filepath
    except Exception as e:
        print(f"‚ùå L·ªói l∆∞u snapshot: {e}")
        return None


def log_event(event_type, message, timestamp=None):
    """Ghi log s·ª± ki·ªán h·ªá th·ªëng."""
    try:
        os.makedirs(config.LOGS_DIR, exist_ok=True)
        if timestamp is None:
            timestamp = time.time()

        log_entry = {
            'timestamp': timestamp,
            'datetime': datetime.fromtimestamp(timestamp).isoformat(),
            'event_type': event_type,
            'message': message
        }

        log_file = os.path.join(config.LOGS_DIR, f"events_{datetime.now().strftime('%Y%m%d')}.json")

        logs = []
        if os.path.exists(log_file):
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    logs = json.load(f)
            except json.JSONDecodeError:
                logs = []  # N·∫øu file b·ªã l·ªói, b·∫Øt ƒë·∫ßu m·ªõi

        logs.append(log_entry)

        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(logs, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"‚ùå L·ªói ghi log: {e}")


# ============ MAIN MONITORING SYSTEM CLASS ============
class BabyMonitorSystem:
    """H·ªá th·ªëng gi√°m s√°t em b√© ch√≠nh."""

    def __init__(self):
        self.initialize_components()
        self.reset_counters()

    def initialize_components(self):
        """Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn h·ªá th·ªëng."""
        print("üöÄ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng gi√°m s√°t em b√© th√¥ng minh...")

        # MediaPipe Pose
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=config.POSE_CONFIDENCE_THRESHOLD,
            min_tracking_confidence=0.5
        )

        # Detectors
        self.face_detector = FaceDetector(confidence_threshold=0.7)
        self.yolo_detector = YOLODetector()
        self.baby_classifier = EnhancedBabyPoseClassifier()
        # Kh·ªüi t·∫°o ArcFace model
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.arcface_model = ArcFace(device=device)
        if self.arcface_model is not None:
            print(f"‚úÖ ArcFace model ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng tr√™n {device}")
        else:
            print("‚ùå Kh·ªüi t·∫°o ArcFace model th·∫•t b·∫°i. T·∫Øt nh·∫≠n di·ªán khu√¥n m·∫∑t.")
            self.arcface_model = None

        # T·∫£i c√°c khu√¥n m·∫∑t ƒë√£ bi·∫øt
        self.known_encodings, self.known_names = load_known_faces()

        # K·∫øt n·ªëi video stream
        self.cap = connect_to_stream(ESP32_STREAM_URL)
        if not self.cap or not self.cap.isOpened():
            print(f"‚ùå K·∫øt n·ªëi stream th·∫•t b·∫°i t·∫°i {ESP32_STREAM_URL}. Tho√°t.")
            exit()  # Tho√°t n·∫øu k·∫øt n·ªëi stream th·∫•t b·∫°i

        print("‚úÖ H·ªá th·ªëng s·∫µn s√†ng!")

    def reset_counters(self):
        """Reset counters v√† bi·∫øn theo d√µi."""
        self.frame_count = 0
        self.last_alert_time = {
            'face': 0,
            'baby_dangerous_pose': 0,
            'baby_tummy_time': 0,
            'no_baby': 0,
        }
        self.baby_detected_in_frame = False

    def handle_stranger_alert(self, frame, current_time):
        """K√≠ch ho·∫°t h√†nh ƒë·ªông cho c·∫£nh b√°o ng∆∞·ªùi l·∫°."""
        if current_time - self.last_alert_time['face'] > config.ALERT_INTERVAL:
            print(f"üö® C·∫¢NH B√ÅO: Ng∆∞·ªùi l·∫° ph√°t hi·ªán! ({datetime.now().strftime('%H:%M:%S')})")
            log_event("stranger_detected", "Ng∆∞·ªùi l·∫° ph√°t hi·ªán.")
            self.last_alert_time['face'] = current_time

            snapshot_path = save_alert_snapshot(frame, "stranger_alert", current_time)
            if snapshot_path:
                # S·ª≠ d·ª•ng threading cho upload ƒë·ªÉ tr√°nh blocking main loop
                threading.Thread(target=upload_to_drive, args=(snapshot_path, GOOGLE_DRIVE_FOLDER_ID)).start()
                print(f"üì§ ƒêang upload snapshot l√™n Google Drive: {snapshot_path}")

    def process_face_recognition(self, frame, faces, baby_boxes, current_time):
        """X·ª≠ l√Ω nh·∫≠n di·ªán khu√¥n m·∫∑t s·ª≠ d·ª•ng ArcFace embeddings v√† cosine similarity."""
        for (x, y, w, h) in faces:
            # B·ªè qua khu√¥n m·∫∑t c√≥ th·ªÉ l√† c·ªßa em b√©
            if is_child_face((x, y, w, h), baby_boxes):
                continue

            # Tr√≠ch xu·∫•t v√πng khu√¥n m·∫∑t v·ªõi padding
            padding = 10
            x1 = max(0, x - padding)
            y1 = max(0, y - padding)
            x2 = min(frame.shape[1], x + w + padding)
            y2 = min(frame.shape[0], y + h + padding)

            face_img_cropped_bgr = frame[y1:y2, x1:x2]
            if face_img_cropped_bgr.size == 0:
                continue

            face_img_cropped_rgb = cv2.cvtColor(face_img_cropped_bgr, cv2.COLOR_BGR2RGB)

            # Nh·∫≠n di·ªán khu√¥n m·∫∑t
            name, confidence = recognize_face_arcface(
                face_img_cropped_rgb,
                self.known_encodings,
                self.known_names,
                self.arcface_model,
                config.STRANGER_SIMILARITY_THRESHOLD
            )

            # X·ª≠ l√Ω k·∫øt qu·∫£ nh·∫≠n di·ªán
            label = "Stranger"
            color = (0, 0, 255)  # Red for stranger

            if name:
                label = f"{name} ({confidence:.1%})"
                color = (0, 255, 0)  # Green for known person
            else:
                # Trigger alert for unknown person
                self.handle_stranger_alert(frame, current_time)

            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)
            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    def process_frame(self, frame):
        """Processes a single video frame."""
        current_time = time.time()
        height, width = frame.shape[:2]

        # Convert to RGB for MediaPipe and ArcFace
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Tracking variables for current frame
        baby_detected_current = False
        baby_boxes = []
        baby_status = "khong_xac_dinh"

        # Process person detection (for baby)
        if self.yolo_detector.model:
            results = self.yolo_detector.model(frame)  # YOLO works on BGR
            if results.xyxy and results.xyxy[0].ndim == 2 and results.xyxy[0].shape[1] >= 6:
                people_detections = results.xyxy[0]
            else:
                people_detections = []

            for detection in people_detections:
                x1, y1, x2, y2, conf, cls = detection[:6]
                x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])

                if not is_baby_size([x1, y1, x2, y2], width, height):
                    continue

                baby_boxes.append([x1, y1, x2, y2])
                baby_detected_current = True

                # Extract baby region for pose estimation
                baby_img = rgb_frame[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]
                if baby_img.size == 0:
                    continue

                # Pose analysis
                pose_results = self.pose.process(baby_img)
                if pose_results.pose_landmarks:
                    pose_label, confidence = self.baby_classifier.classify_pose_enhanced(
                        pose_results.pose_landmarks.landmark, x2 - x1, y2 - y1
                    )

                    stable_pose, stable_confidence = self.baby_classifier.update_pose_history(
                        pose_label, confidence
                    )

                    baby_status = stable_pose

                    # Choose color based on pose
                    color = (0, 255, 0)  # Green - normal
                    if baby_status in ["nam_up_180", "nghiem_trong"]:  # "inverted" -> "nghiem_trong" or similar
                        color = (0, 0, 255)  # Red - dangerous
                    elif baby_status == "tummy_time":
                        color = (255, 255, 0)  # Yellow - tummy time

                    # Draw bounding box and pose status
                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                    cv2.putText(
                        frame,
                        f"Baby: {baby_status} ({stable_confidence:.1%})",
                        (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2
                    )
                else:
                    # If no pose landmarks, still draw the baby box
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 165, 255), 2)  # Orange for detected but no pose
                    cv2.putText(
                        frame,
                        "Baby: Pose N/A",
                        (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 165, 255), 2
                    )

        # Process face recognition (for strangers)
        if self.arcface_model and self.arcface_model.app and self.frame_count % config.FACE_PROCESS_INTERVAL == 0:
            faces = self.face_detector.detect_faces(frame)

            self.process_face_recognition(frame, faces, baby_boxes, current_time)

        # Check for "no baby detected" alert
        if not baby_detected_current and self.baby_detected_in_frame:  # Baby was detected, but now isn't
            if current_time - self.last_alert_time['no_baby'] > config.ALERT_INTERVAL:
                print(f"üö® C·∫¢NH B√ÅO: Kh√¥ng th·∫•y em b√©! ({datetime.now().strftime('%H:%M:%S')})")
                log_event("no_baby_detected", "Kh√¥ng th·∫•y em b√©.")
                self.last_alert_time['no_baby'] = current_time
                snapshot_path = save_alert_snapshot(frame, "no_baby_alert", current_time)
                if snapshot_path:
                    threading.Thread(target=upload_to_drive, args=(snapshot_path, GOOGLE_DRIVE_FOLDER_ID)).start()

        # Check for dangerous pose alert
        if baby_detected_current:
            if baby_status in ["nam_up_180", "nam_sap"] and current_time - self.last_alert_time[
                'baby_dangerous_pose'] > config.ALERT_INTERVAL:
                print(f"üö® C·∫¢NH B√ÅO: Em b√© ·ªü t∆∞ th·∫ø nguy hi·ªÉm: {baby_status} ({datetime.now().strftime('%H:%M:%S')})")
                log_event("dangerous_pose", f"Em b√© ·ªü t∆∞ th·∫ø nguy hi·ªÉm: {baby_status}.")
                self.last_alert_time['baby_dangerous_pose'] = current_time
                snapshot_path = save_alert_snapshot(frame, "dangerous_pose_alert", current_time)
                if snapshot_path:
                    threading.Thread(target=upload_to_drive, args=(snapshot_path, GOOGLE_DRIVE_FOLDER_ID)).start()

            # Check for tummy time notification
            elif baby_status == "tummy_time" and current_time - self.last_alert_time[
                'baby_tummy_time'] > config.ALERT_INTERVAL:
                print(f"‚ÑπÔ∏è TH√îNG B√ÅO: Em b√© ƒëang tummy time ({datetime.now().strftime('%H:%M:%S')})")
                log_event("tummy_time", "Em b√© ƒëang tummy time.")
                self.last_alert_time['baby_tummy_time'] = current_time

        self.baby_detected_in_frame = baby_detected_current  # Update status for next frame

        # Display status information
        frame_time_taken = time.time() - current_time  # Time taken to process this frame
        fps = 1.0 / frame_time_taken if frame_time_taken > 0 else 0

        status_lines = [
            f"Baby: {'Yes' if baby_detected_current else 'No'}",
            f"Pose: {baby_status if baby_detected_current else 'N/A'}",
            f"FPS: {fps:.1f}",
            f"Frame: {self.frame_count}",
            f"Time: {datetime.now().strftime('%H:%M:%S')}"
        ]
        for i, text in enumerate(status_lines):
            y_pos = 30 + i * 25
            cv2.putText(frame, text, (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

        cv2.imshow("Baby Monitor", frame)
        self.frame_count += 1

        if cv2.waitKey(1) & 0xFF == ord('q'):
            print("üõë Shutting down system...")
            return False  # Signal to stop loop
        return True  # Signal to continue loop


# Main execution block
if __name__ == "__main__":
    # Ensure necessary directories exist
    os.makedirs(config.SNAPSHOTS_DIR, exist_ok=True)
    os.makedirs(config.LOGS_DIR, exist_ok=True)
    os.makedirs(config.KNOWN_FACES_DIR, exist_ok=True)  # Ensure known_faces is there

    try:
        baby_monitor = BabyMonitorSystem()
        if not baby_monitor.cap or not baby_monitor.cap.isOpened():
            print("‚ùå Exiting due to failed stream connection.")
        else:
            while True:
                ret, frame = baby_monitor.cap.read()
                if not ret:
                    print("‚ùå Could not read frame from stream. Exiting.")
                    break
                if not baby_monitor.process_frame(frame):  # process_frame now returns True/False
                    break  # Break loop if process_frame signals to stop

    except KeyboardInterrupt:
        print("üõë User stopped the program.")
    except Exception as ex:
        print(f"‚ùå System error: {ex}")
        import traceback
        traceback.print_exc()
    finally:
        if 'baby_monitor' in locals() and hasattr(baby_monitor, 'cap') and baby_monitor.cap.isOpened():
            baby_monitor.cap.release()
        if 'baby_monitor' in locals() and hasattr(baby_monitor, 'pose') and baby_monitor.pose:
            baby_monitor.pose.close()
        cv2.destroyAllWindows()
        if 'baby_monitor' in locals():
            print(f"‚úÖ Resources released. Total frames processed: {baby_monitor.frame_count}")
